{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path.cwd().name == \"notebooks\":\n",
    "\t%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_config = [('max_colwidth', 300), ('display.max_columns', 100), ('display.max_colwidth', 300), ('display.max_rows', 500), ('display.float_format', '%.5f')]\n",
    "# pd.set_option(*(*(\"max_colwidth\", 500), *(\"display.max_column\", 150)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import starmap\n",
    "list(starmap(pd.set_option, pandas_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.get_option(\"max_colwidth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1264/2269264499.py:19: DeprecationWarning: Failing to pass a value for the 'fields' parameter is deprecated and will be disallowed in Python 3.15. To create a TypedDict class with 0 fields using the functional syntax, pass an empty dictionary, e.g. `path = TypedDict('path', {})`.\n",
      "  path: TypedDict(\"path\", )\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from types import SimpleNamespace\n",
    "from typing import Dict, List, Union, NewType\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# class ConfigModel(BaseModel):\n",
    "# \ttemplates: None\n",
    "# \tpath: Dict[str, Union[Path, Dict[str, Union[Path, None]], None]]\n",
    "PathType = Dict[str, Union[Path, None]]\n",
    "\n",
    "class ConfigPath(BaseModel):\n",
    "\tpath: Dict[str, Union[Path, PathType, None]]\n",
    "\n",
    "class ConfigPath(BaseModel):\n",
    "\tpath: TypedDict(\"path\", )\n",
    "\t\n",
    "class ConfigModel(BaseModel):\n",
    "\ttemplates: None\n",
    "\tpath: None\n",
    "\n",
    "class Config:\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tself.config_path = Path().cwd() / 'config'\n",
    "\t\n",
    "\tdef _dict_to_paths(self, d: Dict, parent=Path()):\n",
    "\t\tpaths = {}\n",
    "\t\tfor key, value in d.items():\n",
    "\t\t\tcurrent_path = parent / key\n",
    "\t\t\tif isinstance(value, dict):\n",
    "\t\t\t\tpaths[key] = self._dict_to_paths(value, current_path)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpaths[key] = current_path\n",
    "\t\treturn paths\n",
    "\t\n",
    "\tdef _set_paths(self, config: Dict) -> Dict:\n",
    "\t\tconfig[\"path\"] = self._dict_to_paths(config[\"path\"])\n",
    "\t\treturn config\n",
    "\n",
    "\tdef load_config(self) -> ConfigModel:\n",
    "\t\tconfig = {\n",
    "\t\t\tpath.stem: yaml.safe_load(path.read_text())\n",
    "\t\t\tfor path in self.config_path.glob('*.yaml')\n",
    "\t\t}\n",
    "\t\treturn config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'templates': None,\n",
       " 'path': {'data': {'base': 'data/',\n",
       "   'external': 'data/external',\n",
       "   'interim': 'data/interim',\n",
       "   'processed': 'data/processed',\n",
       "   'raw': 'data/raw'},\n",
       "  'notebooks': 'notebooks/',\n",
       "  'scripts': 'scripts/'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "config_dict = config.load_config()\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import dotenv\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_dict_to_namespace(_dict: Dict) -> SimpleNamespace:\n",
    "    if isinstance(_dict, dict):\n",
    "        namespace = SimpleNamespace()\n",
    "        setattr(namespace, 'config', _dict)\n",
    "        for key, value in _dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                setattr(namespace, key, _convert_dict_to_namespace(value))\n",
    "            else:\n",
    "                setattr(namespace, key, value)\n",
    "        return namespace\n",
    "    else:\n",
    "        raise Exception(\"The input \")\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path().cwd() / 'config'\n",
    "\n",
    "def convert_dict_to_namespace(_dict: Dict) -> SimpleNamespace:\n",
    "    if isinstance(_dict, dict):\n",
    "        namespace = SimpleNamespace()\n",
    "        setattr(namespace, 'config', _dict)\n",
    "        for key, value in _dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                setattr(namespace, key, _convert_dict_to_namespace(value))\n",
    "            else:\n",
    "                setattr(namespace, key, value)\n",
    "        return namespace\n",
    "    else:\n",
    "        raise Exception(\"The input must be a dictionary\")\n",
    "    return _dict\n",
    "\n",
    "def process_paths(paths_namespace, base_path, config_path=None):\n",
    "    \"\"\"Recursively process paths, handling only namespace objects\"\"\"\n",
    "    if config_path is None:\n",
    "        config_path = SimpleNamespace()\n",
    "    items = [\n",
    "        (key, getattr(paths_namespace, key))\n",
    "        for key in dir(paths_namespace)\n",
    "        if not key.startswith('_') and not callable(getattr(paths_namespace, key))\n",
    "    ]\n",
    "    for key, value in items:\n",
    "        if hasattr(value, '__dict__'):\n",
    "            nested_namespace = SimpleNamespace()\n",
    "            setattr(config_path, key, process_paths(value, base_path, nested_namespace))\n",
    "        elif isinstance(value, str):\n",
    "            setattr(config_path, key, base_path / value)\n",
    "        else:\n",
    "            setattr(config_path, key, value)\n",
    "    return config_path\n",
    "\n",
    "def set_paths(config):\n",
    "    base_path = Path().cwd()\n",
    "    paths = getattr(config, 'path')  # Use getattr for namespace\n",
    "    config_path = process_paths(paths, base_path)\n",
    "    setattr(config, 'path', config_path)  # Use setattr for namespace\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(config={'templates': None,\n",
       "                  'path': {'data': {'base': 'data/',\n",
       "                    'external': 'data/external',\n",
       "                    'interim': 'data/interim',\n",
       "                    'processed': 'data/processed',\n",
       "                    'raw': 'data/raw'},\n",
       "                   'notebooks': 'notebooks/',\n",
       "                   'scripts': 'scripts/'}},\n",
       "          templates=None,\n",
       "          path=namespace(config={'data': {'base': 'data/',\n",
       "                                  'external': 'data/external',\n",
       "                                  'interim': 'data/interim',\n",
       "                                  'processed': 'data/processed',\n",
       "                                  'raw': 'data/raw'},\n",
       "                                 'notebooks': 'notebooks/',\n",
       "                                 'scripts': 'scripts/'},\n",
       "                         data=namespace(base=PosixPath('/home/leobit/Development/Projects/aws-doc-ragqa/research/data'),\n",
       "                                        config={'base': 'data/',\n",
       "                                                'external': 'data/external',\n",
       "                                                'interim': 'data/interim',\n",
       "                                                'processed': 'data/processed',\n",
       "                                                'raw': 'data/raw'},\n",
       "                                        external=PosixPath('/home/leobit/Development/Projects/aws-doc-ragqa/research/data/external'),\n",
       "                                        interim=PosixPath('/home/leobit/Development/Projects/aws-doc-ragqa/research/data/interim'),\n",
       "                                        processed=PosixPath('/home/leobit/Development/Projects/aws-doc-ragqa/research/data/processed'),\n",
       "                                        raw=PosixPath('/home/leobit/Development/Projects/aws-doc-ragqa/research/data/raw')),\n",
       "                         notebooks=PosixPath('/home/leobit/Development/Projects/aws-doc-ragqa/research/notebooks'),\n",
       "                         scripts=PosixPath('/home/leobit/Development/Projects/aws-doc-ragqa/research/scripts')))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict_n = convert_dict_to_namespace(config_dict)\n",
    "set_paths(config_dict_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/leobit/Development/Projects/aws-doc-ragqa/research/data/external')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict_n.path.data.external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/leobit/Development/Projects/aws-doc-ragqa/research/data/external')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.path.data.external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = ConfigDict(config=\"a\", num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class User(BaseModel):\n",
    "    id: int\n",
    "    name: str = 'Jane Doe'\n",
    "    stats: TypedDict(\"Stats\", {\"age\": int, \"height\": float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m user.id\n",
      "\u001b[31mNameError\u001b[39m: name 'user' is not defined"
     ]
    }
   ],
   "source": [
    "user.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import pathlib\n",
    "from typing import Union\n",
    "\n",
    "# Third party library imports\n",
    "from pydantic import BaseModel, field_validator\n",
    "\n",
    "cwd = Path().cwd() / 'config'\n",
    "config_file = cwd / \"config.yml\"\n",
    "\n",
    "\n",
    "class App(BaseModel):\n",
    "    \"\"\"App config class.\"\"\"\n",
    "    name: str\n",
    "    version: str\n",
    "\n",
    "\n",
    "class Testing(BaseModel):\n",
    "    \"\"\"Testing config class.\"\"\"\n",
    "    path: pathlib.Path\n",
    "\n",
    "    @field_validator('path')\n",
    "    @classmethod\n",
    "    def transform_path(cls, path: Union[pathlib.Path, str]) -> pathlib.Path:\n",
    "        \"\"\"Transforms a relative path into an absolute path.\"\"\"\n",
    "        \n",
    "        if not isinstance(path, (pathlib.Path | str)):\n",
    "            message = \"Error: Path must be pathlib.Path or str.\"\n",
    "            raise TypeError(message)\n",
    "\n",
    "        path = pathlib.Path(path)\n",
    "\n",
    "        if path.is_absolute():\n",
    "            return path\n",
    "\n",
    "        return cwd / path\n",
    "\n",
    "\n",
    "class Config(BaseModel):\n",
    "    \"\"\"CLI config class.\"\"\"\n",
    "    app: App\n",
    "    testing: Testing\n",
    "\n",
    "\n",
    "def _load_yml_config(path: pathlib.Path):\n",
    "    \"\"\"Classmethod returns YAML config\"\"\"\n",
    "    try:\n",
    "        return yaml.safe_load(path.read_text())\n",
    "\n",
    "    except FileNotFoundError as error:\n",
    "        message = \"Error: yml config file not found.\"\n",
    "        logger.exception(message)\n",
    "        raise FileNotFoundError(error, message) from error\n",
    "\n",
    "\n",
    "Settings = Config(**_load_yml_config(config_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/leobit/Development/Projects/aws-doc-ragqa/research/config/example_date.txt')"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Settings.testing.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/leobit/Development/Projects/aws-doc-ragqa/research/data')"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.path.data.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Evaluation in module weave.flow.eval:\n",
      "\n",
      "class Evaluation(weave.flow.obj.Object)\n",
      " |  Evaluation(*, name: Optional[str] = None, description: Optional[str] = None, ref: Optional[weave.trace.refs.ObjectRef] = None, dataset: typing.Annotated[weave.flow.dataset.Dataset, BeforeValidator(func=<function cast_to_dataset at 0x7eff61655f80>, json_schema_input_type=PydanticUndefined)], scorers: Optional[list[Annotated[Union[weave.trace.op.Op, weave.flow.scorer.Scorer], BeforeValidator(func=<function cast_to_scorer at 0x7eff61656020>, json_schema_input_type=PydanticUndefined)]]] = None, preprocess_model_input: Optional[Callable[[dict], dict]] = None, trials: int = 1, evaluation_name: Union[str, Callable[[weave.trace.weave_client.Call], str], NoneType] = None) -> None\n",
      " |  \n",
      " |  Sets up an evaluation which includes a set of scorers and a dataset.\n",
      " |  \n",
      " |  Calling evaluation.evaluate(model) will pass in rows from a dataset into a model matching\n",
      " |      the names of the columns of the dataset to the argument names in model.predict.\n",
      " |  \n",
      " |  Then it will call all of the scorers and save the results in weave.\n",
      " |  \n",
      " |  If you want to preprocess the rows from the dataset you can pass in a function\n",
      " |  to preprocess_model_input.\n",
      " |  \n",
      " |  Examples:\n",
      " |  \n",
      " |  ```python\n",
      " |  # Collect your examples\n",
      " |  examples = [\n",
      " |      {\"question\": \"What is the capital of France?\", \"expected\": \"Paris\"},\n",
      " |      {\"question\": \"Who wrote 'To Kill a Mockingbird'?\", \"expected\": \"Harper Lee\"},\n",
      " |      {\"question\": \"What is the square root of 64?\", \"expected\": \"8\"},\n",
      " |  ]\n",
      " |  \n",
      " |  # Define any custom scoring function\n",
      " |  @weave.op()\n",
      " |  def match_score1(expected: str, model_output: dict) -> dict:\n",
      " |      # Here is where you'd define the logic to score the model output\n",
      " |      return {'match': expected == model_output['generated_text']}\n",
      " |  \n",
      " |  @weave.op()\n",
      " |  def function_to_evaluate(question: str):\n",
      " |      # here's where you would add your LLM call and return the output\n",
      " |      return  {'generated_text': 'Paris'}\n",
      " |  \n",
      " |  # Score your examples using scoring functions\n",
      " |  evaluation = Evaluation(\n",
      " |      dataset=examples, scorers=[match_score1]\n",
      " |  )\n",
      " |  \n",
      " |  # Start tracking the evaluation\n",
      " |  weave.init('intro-example')\n",
      " |  # Run the evaluation\n",
      " |  asyncio.run(evaluation.evaluate(function_to_evaluate))\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Evaluation\n",
      " |      weave.flow.obj.Object\n",
      " |      pydantic.main.BaseModel\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  async evaluate(self, model: Union[weave.trace.op.Op, weave.flow.model.Model]) -> dict\n",
      " |  \n",
      " |  async get_eval_results(self, model: Union[weave.trace.op.Op, weave.flow.model.Model]) -> weave.flow.eval.EvaluationResults\n",
      " |  \n",
      " |  model_post_init(self, _Evaluation__context: Any) -> None\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |  \n",
      " |  async predict_and_score(self, model: Union[weave.trace.op.Op, weave.flow.model.Model], example: dict) -> dict\n",
      " |  \n",
      " |  async summarize(self, eval_table: weave.flow.eval.EvaluationResults) -> dict\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_obj(obj: weave.trace.vals.WeaveObject) -> Self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_output_key': typing.Literal['output', 'model_outp...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __private_attributes__ = {'_output_key': ModelPrivateAttr(default='out...\n",
      " |  \n",
      " |  __pydantic_complete__ = True\n",
      " |  \n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |  \n",
      " |  __pydantic_core_schema__ = {'function': {'function': <bound method Obj...\n",
      " |  \n",
      " |  __pydantic_custom_init__ = False\n",
      " |  \n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |  \n",
      " |  __pydantic_fields__ = {'dataset': FieldInfo(annotation=Dataset, requir...\n",
      " |  \n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |  \n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |  \n",
      " |  __pydantic_post_init__ = 'model_post_init'\n",
      " |  \n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |  \n",
      " |  __pydantic_setattr_handlers__ = {}\n",
      " |  \n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"Evaluation\", validator...\n",
      " |  \n",
      " |  __signature__ = <Signature (*, name: Optional[str] = None, descr...ve_...\n",
      " |  \n",
      " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'forbid', 'i...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from weave.flow.obj.Object:\n",
      " |  \n",
      " |  __getattribute__ = pydantic_getattribute(self: pydantic.main.BaseModel, name: str) -> Any from weave.trace.vals\n",
      " |  \n",
      " |  __str__ = __repr__(self) -> 'str'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from weave.flow.obj.Object:\n",
      " |  \n",
      " |  from_uri(uri: str, *, objectify: bool = True) -> Self\n",
      " |      Create an object instance from a Weave URI.\n",
      " |      \n",
      " |      Args:\n",
      " |          uri (str): The Weave URI pointing to the object.\n",
      " |          objectify (bool): Whether to objectify the result. Defaults to True.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Self: An instance of the class created from the URI.\n",
      " |      \n",
      " |      Raises:\n",
      " |          NotImplementedError: If the class doesn't implement the required\n",
      " |              methods for deserialization.\n",
      " |      \n",
      " |      Examples:\n",
      " |          ```python\n",
      " |          obj = MyObject.from_uri(\"weave:///entity/project/object:digest\")\n",
      " |          ```\n",
      " |  \n",
      " |  handle_relocatable_object(v: Any, handler: pydantic_core.core_schema.ValidatorFunctionWrapHandler, info: pydantic_core.core_schema.ValidationInfo) -> Any\n",
      " |      Handle validation of relocatable objects including ObjectRef and WeaveObject.\n",
      " |      \n",
      " |      This validator handles special cases where the input is an ObjectRef or\n",
      " |      WeaveObject that needs to be properly converted to a standard Object instance.\n",
      " |      It ensures that references are preserved and that ignored types are handled\n",
      " |      correctly during the validation process.\n",
      " |      \n",
      " |      Args:\n",
      " |          v (Any): The value to validate.\n",
      " |          handler (ValidatorFunctionWrapHandler): The standard pydantic validation handler.\n",
      " |          info (ValidationInfo): Validation context information.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Any: The validated object instance.\n",
      " |      \n",
      " |      Examples:\n",
      " |          This method is called automatically during object creation and validation.\n",
      " |          It handles cases like:\n",
      " |          ```python\n",
      " |          # When an ObjectRef is passed\n",
      " |          obj = MyObject(some_object_ref)\n",
      " |      \n",
      " |          # When a WeaveObject is passed\n",
      " |          obj = MyObject(some_weave_object)\n",
      " |          ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from weave.flow.obj.Object:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |  \n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |  \n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |  \n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __init__(self, /, **data: 'Any') -> 'None'\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      " |      validated to form a valid model.\n",
      " |      \n",
      " |      `self` is explicitly positional-only to allow `self` as a field name.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |  \n",
      " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |  \n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_args__(self) -> '_repr.ReprArgs'\n",
      " |  \n",
      " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
      " |      Returns the string representation of a recursive object.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |  \n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |  \n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |      \n",
      " |      If you need `include` or `exclude`, use:\n",
      " |      \n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |  \n",
      " |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_copy`](../concepts/serialization.md#model_copy)\n",
      " |      \n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! note\n",
      " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
      " |          might have unexpected side effects if you store anything in it, on top of the model\n",
      " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
      " |      \n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |  \n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump`](../concepts/serialization.md#modelmodel_dump)\n",
      " |      \n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |  \n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump_json`](../concepts/serialization.md#modelmodel_dump_json)\n",
      " |      \n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
      " |  \n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
      " |  \n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |      \n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |  \n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called.\n",
      " |      \n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |      \n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by pydantic.\n",
      " |  \n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |  \n",
      " |  from_orm(obj: 'Any') -> 'Self'\n",
      " |  \n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |      \n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      \n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |  \n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]'\n",
      " |      Generates a JSON schema for a model class.\n",
      " |      \n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |  \n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |      \n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |      \n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |  \n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None'\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |      \n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |      \n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |  \n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
      " |      Validate a pydantic model instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |  \n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
      " |      \n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |  \n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |  \n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |  \n",
      " |  parse_obj(obj: 'Any') -> 'Self'\n",
      " |  \n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
      " |  \n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
      " |  \n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
      " |  \n",
      " |  validate(value: 'Any') -> 'Self'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |  \n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __pydantic_extra__\n",
      " |  \n",
      " |  __pydantic_fields_set__\n",
      " |  \n",
      " |  __pydantic_private__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __pydantic_root_model__ = False\n",
      " |  \n",
      " |  model_computed_fields = {}\n",
      " |  \n",
      " |  model_fields = {'dataset': FieldInfo(annotation=Dataset, required=True...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "\n",
    "help(weave.Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.llama_dataset import download_llama_dataset\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# download and install dependencies for benchmark dataset\n",
    "rag_dataset, documents = download_llama_dataset(\n",
    "    \"EvaluatingLlmSurveyPaperDataset\", \"./data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Large Language Models: A\n",
      "Comprehensive Survey\n",
      "Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi\n",
      "Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong\n",
      "Tianjin University\n",
      "{guozishan, rrjin, liuc_09, yuki_731, shidan, supryadi}@tju.edu.cn\n",
      "{linhaoyu, yan_liu, jiaxuanlee, xbj1355, dyxiong}@tju.edu.cn\n",
      "Abstract\n",
      "Large language models (LLMs) have demonstrated remarkable capabilities\n",
      "across a broad spectrum of tasks. They have attracted significant attention\n",
      "and been deployed in numerous downstream applications. Nevertheless, akin\n",
      "to a double-edged sword, LLMs also present potential risks. They could\n",
      "suffer from private data leaks or yield inappropriate, harmful, or misleading\n",
      "content. Additionally, the rapid progress of LLMs raises concerns about the\n",
      "potential emergence of superintelligent systems without adequate safeguards.\n",
      "To effectively capitalize on LLM capacities as well as ensure their safe and\n",
      "beneficial development, it is critical to conduct a rigorous and comprehensive\n",
      "evaluation of LLMs.\n",
      "This survey endeavors to offer a panoramic perspective on the evaluation\n",
      "of LLMs. We categorize the evaluation of LLMs into three major groups:\n",
      "knowledgeandcapabilityevaluation, alignmentevaluationandsafetyevaluation.\n",
      "In addition to the comprehensive review on the evaluation methodologies and\n",
      "benchmarks on these three aspects, we collate a compendium of evaluations\n",
      "pertaining to LLMs performance in specialized domains, and discuss the\n",
      "construction of comprehensive evaluation platforms that cover LLM evaluations\n",
      "on capabilities, alignment, safety, and applicability.\n",
      "We hope that this comprehensive overview will stimulate further research\n",
      "interests in the evaluation of LLMs, with the ultimate goal of making evaluation\n",
      "serve as a cornerstone in guiding the responsible development of LLMs. We\n",
      "envision that this will channel their evolution into a direction that maximizes\n",
      "societal benefit while minimizing potential risks. A curated list of related\n",
      "papers has been publicly available at a GitHub repository.1\n",
      "Equal contribution\n",
      "Corresponding author.\n",
      "1https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers\n",
      "1arXiv:2310.19736v3  [cs.CL]  25 Nov 2023\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reference_contexts = rag_dataset.to_pandas().iloc[0].reference_contexts\n",
    "for rc in reference_contexts:\n",
    "    print(rc)\n",
    "    print(\"-\" * 100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import asdict, dataclass\n",
    "\n",
    "@dataclass\n",
    "class X:\n",
    "    i: int = 2\n",
    "\n",
    "x = X(i=42)\n",
    "x.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg: float = getattr(cfg.app, model_provider)\n",
    "if hasattr(model_cfg, \"similarity_top_k\"):\n",
    "\tsimilarity_top_k = model_cfg.similarity_top_k\n",
    "if hasattr(model_cfg, \"temperature\"):\n",
    "\ttemperature = model_cfg.temperature\n",
    "if hasattr(model_cfg, \"context_size\"):\n",
    "\tcontext_size = model_cfg.context_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leobit/Development/Projects/aws-doc-ragqa/research\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_provider = \"gemini\"\n",
    "model_cfg = getattr(cfg.app, model_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if hasattr(model_cfg, \"similarity_top_k\"):\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-doc-ragqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
