{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Documnetation RAG-QA\n",
    "\n",
    "## Notebook Purpose\n",
    "Data Anaysis for Chunking and Indexing\n",
    " \n",
    "## Tasks\n",
    "- Data Chunk\n",
    "- Indexing\n",
    "\n",
    "## Notable TODOs:\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leobit/Development/Projects/aws-doc-ragqa/research\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "if Path.cwd().name == \"notebooks\":\n",
    "\t%cd ..\n",
    "\n",
    "## Extensions\n",
    "if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "\t%load_ext autoreload\n",
    "else:\n",
    "\t%reload_ext autoreload\n",
    "\n",
    "if 'dotenv' not in ipython.extension_manager.loaded:\n",
    "\t%load_ext dotenv\n",
    "else:\n",
    "\t%reload_ext dotenv\n",
    "\n",
    "# if 'cudf.pandas' not in get_ipython().extension_manager.loaded:\n",
    "# \t%load_ext cudf.pandas\n",
    "\n",
    "%autoreload 2\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config / Utils\n",
    "import config as cfg\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from utils import save_obj, load_obj, run_api\n",
    "\n",
    "@register_cell_magic\n",
    "def pybash(line, cell):\n",
    "\t'''Runs a magic bash with Python Variables'''\n",
    "\tipython.run_cell_magic('bash', '--no-raise-error', cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Infomation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## GLOBAL INFO\n",
      "Conda Python Version: 3.12.9.final.0\n",
      "Conda Base Path: /opt/miniconda3\n",
      "Conda Base Version: 25.1.1\n",
      "\n",
      "## ENVIRONMENT INFO\n",
      "Active Environment: aws-doc-ragqa\n",
      "Environment Python Version: Python 3.11.13\n",
      "Environment Python Path: /opt/miniconda3/envs/aws-doc-ragqa/bin/python\n",
      "Environment IPython Version: 9.1.0\n",
      "Environment IPykernel Version: 6.29.5\n",
      "\n",
      "## GPU INFO:\n",
      "CUDA Device Initialized "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leobit/Development/Projects/aws-doc-ragqa/research/scripts/notebook_info.sh: line 21: numba: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GPU Info: Failed to initialize NVML: N/A\n",
      "Failed to properly shut down NVML: N/A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "bash {cfg.path.scripts}/notebook_info.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "- 'What is SageMaker?',\n",
    "- 'What are all AWS regions where SageMaker is available?',\n",
    "- 'How to check if an endpoint is KMS encrypted?',\n",
    "- 'What are SageMaker Geospatial capabilities?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat with Amazon Q Developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mWelcome to \u001b[96mAmazon Q\u001b[39m!\u001b[22m\n",
      "\n",
      "💡 Get notified whenever Q CLI finishes responding. Just run \u001b[92mq settings\n",
      "    chat.enableNotifications true\u001b[39m\n",
      "\n",
      "\u001b[92m/help\u001b[90m all commands\n",
      "\u001b[92mctrl + j\u001b[90m new lines\n",
      "\u001b[92mctrl + s\u001b[90m fuzzy search\n",
      "\u001b[39m\n",
      "\u001b[38;5;8m\u001b[39m\n",
      "\u001b[39m\u001b[38;5;14m🤖 You are chatting with claude-4-sonnet\n",
      "\u001b[39m\n",
      "\u001b[?25h\u001b[39m\u001b[0m\n",
      "(To exit the CLI, press Ctrl+C or Ctrl+D again or type \u001b[38;5;10m/quit\u001b[39m)\n",
      "\n",
      "\u001b[1G\u001b[0m\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "q chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "> What is SageMaker?\n",
    "\n",
    "> Amazon SageMaker is a fully managed machine learning service provided by AWS that enables developers and data scientists to build, train, and deploy machine learning models at scale.\n",
    "\n",
    "Here are the key components and capabilities of SageMaker:\n",
    "\n",
    "Core Features:\n",
    "• **SageMaker Studio** - Integrated development environment for ML workflows\n",
    "• **SageMaker Notebooks** - Managed Jupyter notebook instances for data exploration and model development\n",
    "• **Built-in Algorithms** - Pre-built ML algorithms optimized for performance and scale\n",
    "• **Custom Training** - Support for custom algorithms using popular frameworks like TensorFlow, PyTorch, and scikit-learn\n",
    "\n",
    "Training and Tuning:\n",
    "• **Automatic Model Tuning** - Hyperparameter optimization to find the best model configuration\n",
    "• **Distributed Training** - Scale training across multiple instances\n",
    "• **Spot Training** - Use EC2 Spot instances to reduce training costs\n",
    "\n",
    "Deployment Options:\n",
    "• **Real-time Endpoints** - Low-latency inference for real-time predictions\n",
    "• **Batch Transform** - Process large datasets for batch predictions\n",
    "• **Multi-Model Endpoints** - Host multiple models on a single endpoint\n",
    "• **Serverless Inference** - Pay-per-use inference without managing infrastructure\n",
    "\n",
    "Additional Services:\n",
    "• **SageMaker Pipelines** - MLOps workflows for automating ML processes\n",
    "• **SageMaker Feature Store** - Centralized repository for ML features\n",
    "• **SageMaker Model Registry** - Version control and governance for ML models\n",
    "• **SageMaker Clarify** - Detect bias and explain model predictions\n",
    "• **SageMaker Data Wrangler** - Visual data preparation tool\n",
    "\n",
    "SageMaker handles the underlying infrastructure, allowing you to focus on the ML aspects rather than managing servers, scaling, or deployment complexities. It integrates well with other AWS services and supports the entire ML lifecycle from data preparation to model monitoring in production.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "> What are all AWS regions where SageMaker is available?\n",
    "\n",
    "> Based on my testing, SageMaker is available in most AWS regions. Here are the regions where Amazon SageMaker is currently available:\n",
    "\n",
    "US Regions:\n",
    "• us-east-1 (N. Virginia)\n",
    "• us-east-2 (Ohio)\n",
    "• us-west-1 (N. California)\n",
    "• us-west-2 (Oregon)\n",
    "\n",
    "Europe Regions:\n",
    "• eu-central-1 (Frankfurt)\n",
    "• eu-west-1 (Ireland)\n",
    "• eu-west-2 (London)\n",
    "• eu-west-3 (Paris)\n",
    "• eu-north-1 (Stockholm)\n",
    "\n",
    "Asia Pacific Regions:\n",
    "• ap-northeast-1 (Tokyo)\n",
    "• ap-northeast-2 (Seoul)\n",
    "• ap-northeast-3 (Osaka)\n",
    "• ap-southeast-1 (Singapore)\n",
    "• ap-southeast-2 (Sydney)\n",
    "• ap-south-1 (Mumbai)\n",
    "\n",
    "Other Regions:\n",
    "• ca-central-1 (Canada Central)\n",
    "• sa-east-1 (São Paulo)\n",
    "\n",
    "Important Notes:\n",
    "• SageMaker is available in most major AWS regions, but not all features may be available in every region\n",
    "• Some newer SageMaker features or instance types might be rolled out to different regions at different times\n",
    "• For the most current and detailed region availability, including specific feature availability, check the AWS Regional Services page or the SageMaker documentation\n",
    "• Some specialized regions (like AWS GovCloud or China regions) may have different availability\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "> How to check if an endpoint is KMS Encrypted by SAGEMAKER_ENDPOINT_CONFIGURATION_KMS_KEY_CONFIGURED?\n",
    "\n",
    "> You want to understand what the SAGEMAKER_ENDPOINT_CONFIGURATION_KMS_KEY_CONFIGURED compliance rule specifically checks for.\n",
    "\n",
    "This AWS Config rule evaluates whether Amazon SageMaker endpoint configurations have KMS encryption configured. Here's what it specifically checks:\n",
    "\n",
    "## What the Rule Validates\n",
    "\n",
    "The rule checks if a SageMaker endpoint configuration has the KmsKeyId parameter specified, which enables encryption at rest for:\n",
    "• Model artifacts\n",
    "• Data captured for monitoring\n",
    "• Any other data stored by the endpoint\n",
    "\n",
    "## Rule Details\n",
    "\n",
    "• **Rule Name**: SAGEMAKER_ENDPOINT_CONFIGURATION_KMS_KEY_CONFIGURED\n",
    "• **Rule Type**: AWS Managed Config Rule\n",
    "• **Resource Type**: AWS::SageMaker::EndpointConfig\n",
    "• **Trigger**: Configuration changes and periodic evaluations\n",
    "\n",
    "## Compliance Criteria\n",
    "\n",
    "An endpoint configuration is COMPLIANT when:\n",
    "• The KmsKeyId field is present and contains a valid KMS key ARN or alias\n",
    "• The KMS key exists and is accessible\n",
    "\n",
    "An endpoint configuration is NON_COMPLIANT when:\n",
    "• The KmsKeyId field is missing, null, or empty\n",
    "• The specified KMS key doesn't exist or isn't accessible\n",
    "\n",
    "## What Gets Checked\n",
    "\n",
    "The rule examines the endpoint configuration JSON structure for:\n",
    "json\n",
    "{\n",
    "\t\"EndpointConfigName\": \"example-config\",\n",
    "\t\"ProductionVariants\": [...],\n",
    "\t\"KmsKeyId\": \"arn:aws:kms:region:account:key/key-id\"  // This field\n",
    "}\n",
    "\n",
    "\n",
    "## Key Points\n",
    "\n",
    "• This rule only checks the configuration - not whether endpoints using that configuration are actually deployed\n",
    "• It validates the presence of KMS encryption settings, not the actual encryption of data\n",
    "• The rule runs when endpoint configurations are created, modified, or during periodic evaluations\n",
    "• It helps ensure compliance with data encryption requirements and security policies\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "> What are SageMaker Geospatial capabilities?\n",
    "\n",
    "> Amazon SageMaker Geospatial capabilities provide a comprehensive set of tools for processing, analyzing, and visualizing geospatial data using machine learning. Here are the key capabilities:\n",
    "\n",
    "## Core Geospatial Features\n",
    "\n",
    "Earth Observation Jobs\n",
    "• Process satellite imagery and geospatial datasets at scale\n",
    "• Built-in algorithms for common geospatial tasks like land cover classification, crop monitoring, and change detection\n",
    "• Support for popular satellite data sources (Sentinel, Landsat, etc.)\n",
    "\n",
    "Geospatial Data Processing\n",
    "• Raster and vector data processing capabilities\n",
    "• Temporal analysis for time-series geospatial data\n",
    "• Coordinate system transformations and projections\n",
    "• Data fusion from multiple geospatial sources\n",
    "\n",
    "## Pre-built Algorithms\n",
    "\n",
    "Land Cover and Land Use\n",
    "• Automatic classification of terrain types\n",
    "• Urban development monitoring\n",
    "• Deforestation tracking\n",
    "\n",
    "Agriculture and Crop Monitoring\n",
    "• Crop health assessment using vegetation indices\n",
    "• Yield prediction models\n",
    "• Irrigation and water stress analysis\n",
    "\n",
    "Environmental Monitoring\n",
    "• Flood detection and mapping\n",
    "• Wildfire monitoring and risk assessment\n",
    "• Air quality analysis\n",
    "\n",
    "## Integration Capabilities\n",
    "\n",
    "Data Sources\n",
    "• Direct integration with AWS Open Data (satellite imagery)\n",
    "• Support for custom geospatial datasets\n",
    "• Integration with Amazon S3 for data storage\n",
    "\n",
    "Visualization\n",
    "• Built-in map visualization tools\n",
    "• Integration with popular GIS tools\n",
    "• Export capabilities for further analysis\n",
    "\n",
    "ML Integration\n",
    "• Seamless integration with SageMaker's ML capabilities\n",
    "• Custom model training on geospatial data\n",
    "• AutoML for geospatial use cases\n",
    "\n",
    "## APIs and SDKs\n",
    "\n",
    "• Python SDK for geospatial operations\n",
    "• REST APIs for programmatic access\n",
    "• Jupyter notebook integration for interactive analysis\n",
    "\n",
    "SageMaker Geospatial essentially democratizes access to satellite data analysis and geospatial ML, making it easier for organizations to derive insights from Earth observation data without requiring deep expertise in geospatial processing.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Gold Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_gold_qa = [\n",
    "\t{\n",
    "\t\t\"id\": \"0\",\n",
    "\t\t\"query\": \"What is SageMaker?\",\n",
    "\t\t\"ground_truth\": \"Amazon SageMaker is a fully managed machine learning service provided by AWS that enables developers and data scientists to build, train, and deploy machine learning models at scale.\",\n",
    "\t},\n",
    "\t{\n",
    "\t\t\"id\": \"1\",\n",
    "\t\t\"query\": \"What are all AWS regions where SageMaker is available?\",\n",
    "\t\t\"ground_truth\": \"This question cannot be answered from the given source.\",\n",
    "\t},\n",
    "\t{\n",
    "\t\t\"id\": \"2\",\n",
    "\t\t\"query\": \"How to check if an endpoint is KMS encrypted?\",\n",
    "\t\t\"ground_truth\": \"Using the AWS Managed Config rule SAGEMAKER_ENDPOINT_CONFIGURATION_KMS_KEY_CONFIGURED to check if KMS is configured for Amazon SageMaker endpoint configuration.\",\n",
    "\t},\n",
    "\t{\n",
    "\t\t\"id\": \"3\",\n",
    "\t\t\"query\": \"What are SageMaker Geospatial capabilities?\",\n",
    "\t\t\"ground_truth\": \"This question cannot be answered from the given source as the file only informs about Geospatial Capabilities roles, and not the concept or features.\",\n",
    "\t},\n",
    "]\n",
    "save_obj(eval_gold_qa, cfg.path.data.processed / \"evaluation_gold_qa_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions Evaluation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gql.transport.requests:>>> {\"query\": \"query DefaultEntity {\\n  viewer {\\n    username\\n    defaultEntity {\\n      name\\n    }\\n  }\\n}\"}\n",
      "INFO:gql.transport.requests:<<< {\"data\":{\"viewer\":{\"username\":\"leoaieng\",\"defaultEntity\":{\"name\":\"leoaieng-lb\"}}}}\n",
      "INFO:gql.transport.requests:>>> {\"query\": \"query DefaultEntity {\\n  viewer {\\n    username\\n    defaultEntity {\\n      name\\n    }\\n  }\\n}\"}\n",
      "INFO:gql.transport.requests:<<< {\"data\":{\"viewer\":{\"username\":\"leoaieng\",\"defaultEntity\":{\"name\":\"leoaieng-lb\"}}}}\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: leoaieng.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/weave\n",
      "INFO:weave.trace.init_message:Logged in as Weights & Biases user: leoaieng.\n",
      "View Weave data at https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/weave\n"
     ]
    }
   ],
   "source": [
    "from aws_doc_ragqa.rag_pipeline import RagPipeline\n",
    "rag_pipe = RagPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9ec1410b7a13\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "docker start {cfg.app.docker.container}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given with relevant information about AWS Documentation. Answer the user query only based on the information provided. Don't make up stuff.\n",
      "\n",
      "User Query: == QUERY ==\n",
      "Context: == CONTEXT ==\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "## Prompt Template\n",
    "rag_pipe = RagPipeline()\n",
    "prompt_template = rag_pipe.get_template()\n",
    "prompt = prompt_template.format(\n",
    "    context_str=\"== CONTEXT ==\",\n",
    "    query_str=\"== QUERY ==\"\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_query = \"What is SageMaker?\"\n",
    "llm_judge = rag_pipe.set_llm_judge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1/exists \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1/points/search \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': \"Based on the provided information, Amazon SageMaker is a fully managed service that enables developers and data scientists to build, train, and deploy machine learning (ML) models. It offers capabilities for every stage of the machine learning lifecycle, from model building and experimentation to deployment and management.\\n\\nThe context doesn't provide a comprehensive definition of SageMaker, but it does highlight some key features and use cases, such as:\\n\\n1. It's a fully managed service for machine learning.\\n2. It provides tools for building, training, and deploying ML models.\\n3. It offers project management capabilities to help teams collaborate more efficiently and maintain code consistency.\\n4. SageMaker Projects can help organize all entities of the ML lifecycle under one project.\\n5. It provides pre-built templates and allows for custom templates to set up ML infrastructure.\\n6. It can integrate with various tools and services for CI/CD, including both AWS-native services and third-party tools.\"}\n"
     ]
    }
   ],
   "source": [
    "response = rag_pipe.predict(eval_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1/points/search \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name: examples-sagemaker.md\n",
      "category: tutorial\n",
      "header_path: /\n",
      "\n",
      "# Working with Amazon SageMaker<a name=\"examples-sagemaker\"></a>\n",
      "\n",
      " Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning \\(ML\\) models\\. See the following resources for complete code examples with instructions\\.\n",
      "\n",
      " [Link to Github](https://github.com/awsdocs/aws-doc-sdk-examples/tree/master/javav2/example_code/sagemaker) \n",
      "\n",
      " [Link to AWS Code Sample Catalog](http://docs.aws.amazon.com/code-samples/latest/catalog/code-catalog-javav2-example_code-sagemaker.html)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "file_name: examples-sagemaker.md\n",
      "category: tutorial\n",
      "header_path: /\n",
      "\n",
      "# Working with Amazon SageMaker<a name=\"examples-sagemaker\"></a>\n",
      "\n",
      " Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning \\(ML\\) models\\. See the following resources for complete code examples with instructions\\.\n",
      "\n",
      " [Link to Github](https://github.com/awsdocs/aws-doc-sdk-examples/tree/master/javav2/example_code/sagemaker) \n",
      "\n",
      " [Link to AWS Code Sample Catalog](http://docs.aws.amazon.com/code-samples/latest/catalog/code-catalog-javav2-example_code-sagemaker.html)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "file_name: examples-sagemaker.md\n",
      "category: tutorial\n",
      "header_path: /\n",
      "\n",
      "# Working with Amazon SageMaker<a name=\"examples-sagemaker\"></a>\n",
      "\n",
      " Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning \\(ML\\) models\\. See the following resources for complete code examples with instructions\\.\n",
      "\n",
      " [Link to Github](https://github.com/awsdocs/aws-doc-sdk-examples/tree/master/javav2/example_code/sagemaker) \n",
      "\n",
      " [Link to AWS Code Sample Catalog](http://docs.aws.amazon.com/code-samples/latest/catalog/code-catalog-javav2-example_code-sagemaker.html)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "file_name: sagemaker-projects-whatis.md\n",
      "category: concepts\n",
      "header_path: /What is a SageMaker Project?<a name=\"sagemaker-projects-whatis\"></a>/\n",
      "\n",
      "## When Should You Use a SageMaker Project?<a name=\"sagemaker-projects-when\"></a>\n",
      "\n",
      "While notebooks are helpful for model building and experimentation, a team of data scientists and ML engineers sharing code needs a more scalable way to maintain code consistency and strict version control\\.\n",
      "\n",
      "Every organization has its own set of standards and practices that provide security and governance for its AWS environment\\. SageMaker provides a set of first\\-party templates for organizations that want to quickly get started with ML workflows and CI/CD\\. The templates include projects that use AWS\\-native services for CI/CD, such as AWS CodeBuild, AWS CodePipeline, and AWS CodeCommit\\. The templates also offer the option to create projects that use third\\-party tools, such as Jenkins and GitHub\\. For a list of the project templates that SageMaker provides, see [Use SageMaker\\-Provided Project Templates](sagemaker-projects-templates-sm.md)\\.\n",
      "\n",
      "Organizations often need tight control over the MLOps resources that they provision and manage\\. Such responsibility assumes certain tasks, including configuring IAM roles and policies, enforcing resource tags, enforcing encryption, and decoupling resources across multiple accounts\\. SageMaker Projects can support all these tasks through custom template offerings where organizations use AWS CloudFormation templates to define the resources needed for an ML workflow\\. Data Scientists can choose a template to bootstrap and pre\\-configure their ML workflow\\. These custom templates are created as Service Catalog products and you can provision them in the Studio UI under **Organization Templates**\\. The Service Catalog is a service that helps organizations create and manage catalogs of products that are approved for use on AWS\\. For more information about creating custom templates, see [Build Custom SageMaker Project Templates – Best Practices](http://aws.amazon.com/blogs/machine-learning/build-custom-sagemaker-project-templates-best-practices/)\\.\n",
      "\n",
      "SageMaker Projects can help you manage your Git repositories so that you can collaborate more efficiently across teams, ensure code consistency, and support CI/CD\\. SageMaker Projects can help you with the following tasks:\n",
      "+ Organize all entities of the ML lifecycle under one project\\.\n",
      "+ Establish a single\\-click approach to set up standard ML infrastructure for model training and deployment that incorporates best practices\\.\n",
      "+ Create and share templates for ML infrastructure to serve multiple use cases\\.\n",
      "+ Leverage SageMaker\\-provided pre\\-built templates to quickly start focusing on model building, or create custom templates with organization\\-specific resources and guidelines\\.\n",
      "+ Integrate with tools of your choice by extending the project templates\\. For an example, see [Create a SageMaker Project to integrate with GitLab and GitLab Pipelines](http://aws.amazon.com/blogs/machine-learning/build-mlops-workflows-with-amazon-sagemaker-projects-gitlab-and-gitlab-pipelines/)\\.\n",
      "+ Organize all entities of the ML lifecycle under one project\\.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "file_name: sagemaker-projects-whatis.md\n",
      "category: concepts\n",
      "header_path: /What is a SageMaker Project?<a name=\"sagemaker-projects-whatis\"></a>/\n",
      "\n",
      "## When Should You Use a SageMaker Project?<a name=\"sagemaker-projects-when\"></a>\n",
      "\n",
      "While notebooks are helpful for model building and experimentation, a team of data scientists and ML engineers sharing code needs a more scalable way to maintain code consistency and strict version control\\.\n",
      "\n",
      "Every organization has its own set of standards and practices that provide security and governance for its AWS environment\\. SageMaker provides a set of first\\-party templates for organizations that want to quickly get started with ML workflows and CI/CD\\. The templates include projects that use AWS\\-native services for CI/CD, such as AWS CodeBuild, AWS CodePipeline, and AWS CodeCommit\\. The templates also offer the option to create projects that use third\\-party tools, such as Jenkins and GitHub\\. For a list of the project templates that SageMaker provides, see [Use SageMaker\\-Provided Project Templates](sagemaker-projects-templates-sm.md)\\.\n",
      "\n",
      "Organizations often need tight control over the MLOps resources that they provision and manage\\. Such responsibility assumes certain tasks, including configuring IAM roles and policies, enforcing resource tags, enforcing encryption, and decoupling resources across multiple accounts\\. SageMaker Projects can support all these tasks through custom template offerings where organizations use AWS CloudFormation templates to define the resources needed for an ML workflow\\. Data Scientists can choose a template to bootstrap and pre\\-configure their ML workflow\\. These custom templates are created as Service Catalog products and you can provision them in the Studio UI under **Organization Templates**\\. The Service Catalog is a service that helps organizations create and manage catalogs of products that are approved for use on AWS\\. For more information about creating custom templates, see [Build Custom SageMaker Project Templates – Best Practices](http://aws.amazon.com/blogs/machine-learning/build-custom-sagemaker-project-templates-best-practices/)\\.\n",
      "\n",
      "SageMaker Projects can help you manage your Git repositories so that you can collaborate more efficiently across teams, ensure code consistency, and support CI/CD\\. SageMaker Projects can help you with the following tasks:\n",
      "+ Organize all entities of the ML lifecycle under one project\\.\n",
      "+ Establish a single\\-click approach to set up standard ML infrastructure for model training and deployment that incorporates best practices\\.\n",
      "+ Create and share templates for ML infrastructure to serve multiple use cases\\.\n",
      "+ Leverage SageMaker\\-provided pre\\-built templates to quickly start focusing on model building, or create custom templates with organization\\-specific resources and guidelines\\.\n",
      "+ Integrate with tools of your choice by extending the project templates\\. For an example, see [Create a SageMaker Project to integrate with GitLab and GitLab Pipelines](http://aws.amazon.com/blogs/machine-learning/build-mlops-workflows-with-amazon-sagemaker-projects-gitlab-and-gitlab-pipelines/)\\.\n",
      "+ Organize all entities of the ML lifecycle under one project\\.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes = rag_pipe.retrieve_nodes(eval_query)\n",
    "for i, node in enumerate(nodes):\n",
    "\tprint(node.get_content(metadata_mode=MetadataMode.ALL))\n",
    "\tprint(\"-\" * 100)\n",
    "\tprint(\"-\" * 100)\n",
    "\tprint(\"-\" * 100)\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Correctness:  \n",
    "1 (worst) - 5 (best)\n",
    "\n",
    "Context Relevancy:  \n",
    "0 (worst) - 1 (best)\n",
    "\n",
    "Faithfullness:  \n",
    "0 (worst) - 1 (best)\n",
    "\n",
    "LLM JUDGE:\n",
    "Claude 3.5 Sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS\n",
    "Model: Claude 3 Haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'context_size': 200000,\n",
      "  'embedding_model_id': 'amazon.titan-embed-text-v2:0',\n",
      "  'llm_model_id': 'us.anthropic.claude-3-haiku-20240307-v1:0',\n",
      "  'region': 'us-east-2',\n",
      "  'similarity_top_k': 5,\n",
      "  'temperature': 0.4}\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "pp.pprint(cfg.app.aws.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gql.transport.requests:>>> {\"query\": \"query DefaultEntity {\\n  viewer {\\n    username\\n    defaultEntity {\\n      name\\n    }\\n  }\\n}\"}\n",
      "INFO:gql.transport.requests:<<< {\"data\":{\"viewer\":{\"username\":\"leoaieng\",\"defaultEntity\":{\"name\":\"leoaieng-lb\"}}}}\n",
      "INFO:gql.transport.requests:>>> {\"query\": \"query DefaultEntity {\\n  viewer {\\n    username\\n    defaultEntity {\\n      name\\n    }\\n  }\\n}\"}\n",
      "INFO:gql.transport.requests:<<< {\"data\":{\"viewer\":{\"username\":\"leoaieng\",\"defaultEntity\":{\"name\":\"leoaieng-lb\"}}}}\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: leoaieng.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/weave\n",
      "INFO:weave.trace.init_message:Logged in as Weights & Biases user: leoaieng.\n",
      "View Weave data at https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/weave\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The current model is: None\n",
      "INFO:ragpipeline:Setting AWS Model...\n",
      "INFO:ragpipeline:Setting AWS LLM: us.anthropic.claude-3-haiku-20240307-v1:0, Embedding: amazon.titan-embed-text-v2:0\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/r/call/0198167f-3c01-734f-98b7-2e0aa7049567\n",
      "INFO:weave.trace.op:🍩 https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/r/call/0198167f-3c01-734f-98b7-2e0aa7049567\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 1 of 4 examples\n",
      "WARNING:llama_index.llms.bedrock.utils:Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 10): Too many requests, please wait before trying again..\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 2 of 4 examples\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 3 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 3 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 4 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 4 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"correctness_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"correctness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 3.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"relevancy_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"relevancy\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 0.5\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"faithfullness_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"faithfullness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 8.921520352363586\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n",
      "INFO:weave.flow.eval:Evaluation summary {\n",
      "  \"correctness_evaluator\": {\n",
      "    \"correctness\": {\n",
      "      \"mean\": 3.0\n",
      "    }\n",
      "  },\n",
      "  \"relevancy_evaluator\": {\n",
      "    \"relevancy\": {\n",
      "      \"mean\": 0.5\n",
      "    }\n",
      "  },\n",
      "  \"faithfullness_evaluator\": {\n",
      "    \"faithfullness\": {\n",
      "      \"mean\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"model_latency\": {\n",
      "    \"mean\": 8.921520352363586\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "uv run python aws_doc_ragqa/run_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'context_size': 200000,\n",
      "  'embedding_model_id': 'amazon.titan-embed-text-v2:0',\n",
      "  'llm_model_id': 'us.anthropic.claude-3-haiku-20240307-v1:0',\n",
      "  'region': 'us-east-2',\n",
      "  'similarity_top_k': 3,\n",
      "  'temperature': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "pp.pprint(cfg.app.aws.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gql.transport.requests:>>> {\"query\": \"query DefaultEntity {\\n  viewer {\\n    username\\n    defaultEntity {\\n      name\\n    }\\n  }\\n}\"}\n",
      "INFO:gql.transport.requests:<<< {\"data\":{\"viewer\":{\"username\":\"leoaieng\",\"defaultEntity\":{\"name\":\"leoaieng-lb\"}}}}\n",
      "INFO:gql.transport.requests:>>> {\"query\": \"query DefaultEntity {\\n  viewer {\\n    username\\n    defaultEntity {\\n      name\\n    }\\n  }\\n}\"}\n",
      "INFO:gql.transport.requests:<<< {\"data\":{\"viewer\":{\"username\":\"leoaieng\",\"defaultEntity\":{\"name\":\"leoaieng-lb\"}}}}\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: leoaieng.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/weave\n",
      "INFO:weave.trace.init_message:Logged in as Weights & Biases user: leoaieng.\n",
      "View Weave data at https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/weave\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The current model is: None\n",
      "INFO:ragpipeline:Setting AWS Model...\n",
      "INFO:ragpipeline:Setting AWS LLM: us.anthropic.claude-3-haiku-20240307-v1:0, Embedding: amazon.titan-embed-text-v2:0\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/r/call/01981684-ddfb-74e7-afab-c6fc3118e149\n",
      "INFO:weave.trace.op:🍩 https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/r/call/01981684-ddfb-74e7-afab-c6fc3118e149\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 1 of 4 examples\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 2 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 3 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 3 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 4 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 4 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"correctness_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"correctness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 3.75\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"relevancy_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"relevancy\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 0.8333333333333334\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"faithfullness_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"faithfullness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 8.0090491771698\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n",
      "INFO:weave.flow.eval:Evaluation summary {\n",
      "  \"correctness_evaluator\": {\n",
      "    \"correctness\": {\n",
      "      \"mean\": 3.75\n",
      "    }\n",
      "  },\n",
      "  \"relevancy_evaluator\": {\n",
      "    \"relevancy\": {\n",
      "      \"mean\": 0.8333333333333334\n",
      "    }\n",
      "  },\n",
      "  \"faithfullness_evaluator\": {\n",
      "    \"faithfullness\": {\n",
      "      \"mean\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"model_latency\": {\n",
      "    \"mean\": 8.0090491771698\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "uv run python aws_doc_ragqa/run_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMINI\n",
    "Model: Gemini 1.5 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'embedding_model_id': 'models/embedding-001',\n",
      "  'llm_model_id': 'models/gemini-1.5-flash',\n",
      "  'similarity_top_k': 3,\n",
      "  'temperature': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "pp.pprint(cfg.app.gemini.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gql.transport.requests:>>> {\"query\": \"query DefaultEntity {\\n  viewer {\\n    username\\n    defaultEntity {\\n      name\\n    }\\n  }\\n}\"}\n",
      "INFO:gql.transport.requests:<<< {\"data\":{\"viewer\":{\"username\":\"leoaieng\",\"defaultEntity\":{\"name\":\"leoaieng-lb\"}}}}\n",
      "INFO:gql.transport.requests:>>> {\"query\": \"query DefaultEntity {\\n  viewer {\\n    username\\n    defaultEntity {\\n      name\\n    }\\n  }\\n}\"}\n",
      "INFO:gql.transport.requests:<<< {\"data\":{\"viewer\":{\"username\":\"leoaieng\",\"defaultEntity\":{\"name\":\"leoaieng-lb\"}}}}\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: leoaieng.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/weave\n",
      "INFO:weave.trace.init_message:Logged in as Weights & Biases user: leoaieng.\n",
      "View Weave data at https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/weave\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The current model is: None\n",
      "INFO:ragpipeline:Setting Gemini Model...\n",
      "INFO:ragpipeline:Setting Gemini LLM: models/gemini-1.5-flash, Embedding: models/embedding-001\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/r/call/01981688-0fcb-7628-8d24-b99443feb5b2\n",
      "INFO:weave.trace.op:🍩 https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/r/call/01981688-0fcb-7628-8d24-b99443feb5b2\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1.1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1.1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1.1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1.1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1.1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1.1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1.1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1.1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1.1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1.1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1.1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1.1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 1 of 4 examples\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 2 of 4 examples\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 3 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 3 of 4 examples\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 4 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 4 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"correctness_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"correctness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.75\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"relevancy_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"relevancy\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 0.41666666666666663\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"faithfullness_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"faithfullness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 0.75\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 3.2683340311050415\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n",
      "INFO:weave.flow.eval:Evaluation summary {\n",
      "  \"correctness_evaluator\": {\n",
      "    \"correctness\": {\n",
      "      \"mean\": 1.75\n",
      "    }\n",
      "  },\n",
      "  \"relevancy_evaluator\": {\n",
      "    \"relevancy\": {\n",
      "      \"mean\": 0.41666666666666663\n",
      "    }\n",
      "  },\n",
      "  \"faithfullness_evaluator\": {\n",
      "    \"faithfullness\": {\n",
      "      \"mean\": 0.75\n",
      "    }\n",
      "  },\n",
      "  \"model_latency\": {\n",
      "    \"mean\": 3.2683340311050415\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "uv run python aws_doc_ragqa/run_eval.py --gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'embedding_model_id': 'models/embedding-001',\n",
      "  'llm_model_id': 'models/gemini-1.5-flash',\n",
      "  'similarity_top_k': 5,\n",
      "  'temperature': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "pp.pprint(cfg.app.gemini.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gql.transport.requests:>>> {\"query\": \"query DefaultEntity {\\n  viewer {\\n    username\\n    defaultEntity {\\n      name\\n    }\\n  }\\n}\"}\n",
      "INFO:gql.transport.requests:<<< {\"data\":{\"viewer\":{\"username\":\"leoaieng\",\"defaultEntity\":{\"name\":\"leoaieng-lb\"}}}}\n",
      "INFO:gql.transport.requests:>>> {\"query\": \"query DefaultEntity {\\n  viewer {\\n    username\\n    defaultEntity {\\n      name\\n    }\\n  }\\n}\"}\n",
      "INFO:gql.transport.requests:<<< {\"data\":{\"viewer\":{\"username\":\"leoaieng\",\"defaultEntity\":{\"name\":\"leoaieng-lb\"}}}}\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: leoaieng.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/weave\n",
      "INFO:weave.trace.init_message:Logged in as Weights & Biases user: leoaieng.\n",
      "View Weave data at https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/weave\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The current model is: None\n",
      "INFO:ragpipeline:Setting Gemini Model...\n",
      "INFO:ragpipeline:Setting Gemini LLM: models/gemini-1.5-flash, Embedding: models/embedding-001\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1.1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1.1\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/r/call/0198168a-8a7d-7511-8fa2-4622492fbe3d\n",
      "INFO:weave.trace.op:🍩 https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/r/call/0198168a-8a7d-7511-8fa2-4622492fbe3d\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1.1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1.1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1.1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1.1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1.1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1.1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1.1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1.1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1.1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1.1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 1 of 4 examples\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 2 of 4 examples\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 3 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 3 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 4 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 4 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"correctness_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"correctness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 2.375\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"relevancy_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"relevancy\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 0.44999999999999996\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"faithfullness_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"faithfullness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 3.785370945930481\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n",
      "INFO:weave.flow.eval:Evaluation summary {\n",
      "  \"correctness_evaluator\": {\n",
      "    \"correctness\": {\n",
      "      \"mean\": 2.375\n",
      "    }\n",
      "  },\n",
      "  \"relevancy_evaluator\": {\n",
      "    \"relevancy\": {\n",
      "      \"mean\": 0.44999999999999996\n",
      "    }\n",
      "  },\n",
      "  \"faithfullness_evaluator\": {\n",
      "    \"faithfullness\": {\n",
      "      \"mean\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"model_latency\": {\n",
      "    \"mean\": 3.785370945930481\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "uv run python aws_doc_ragqa/run_eval.py --gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: Gemini 2.5 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'embedding_model_id': 'models/embedding-001',\n",
      "  'llm_model_id': 'models/gemini-2.5-pro',\n",
      "  'similarity_top_k': 3,\n",
      "  'temperature': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "pp.pprint(cfg.app.gemini.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gql.transport.requests:>>> {\"query\": \"query DefaultEntity {\\n  viewer {\\n    username\\n    defaultEntity {\\n      name\\n    }\\n  }\\n}\"}\n",
      "INFO:gql.transport.requests:<<< {\"data\":{\"viewer\":{\"username\":\"leoaieng\",\"defaultEntity\":{\"name\":\"leoaieng-lb\"}}}}\n",
      "INFO:gql.transport.requests:>>> {\"query\": \"query DefaultEntity {\\n  viewer {\\n    username\\n    defaultEntity {\\n      name\\n    }\\n  }\\n}\"}\n",
      "INFO:gql.transport.requests:<<< {\"data\":{\"viewer\":{\"username\":\"leoaieng\",\"defaultEntity\":{\"name\":\"leoaieng-lb\"}}}}\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: leoaieng.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/weave\n",
      "INFO:weave.trace.init_message:Logged in as Weights & Biases user: leoaieng.\n",
      "View Weave data at https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/weave\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The current model is: None\n",
      "INFO:ragpipeline:Setting Gemini Model...\n",
      "INFO:ragpipeline:Setting Gemini LLM: models/gemini-2.5-pro, Embedding: models/embedding-001\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: 🍩 https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/r/call/0198168f-8cc7-753d-b2b1-3ad518bb154b\n",
      "INFO:weave.trace.op:🍩 https://wandb.ai/leoaieng-lb/aws-doc-ragqa-demo/r/call/0198168f-8cc7-753d-b2b1-3ad518bb154b\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1.1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1.1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1.1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:ragpipeline:Preparing to get index...\n",
      "INFO:ragpipeline:The Model provider is already set.\n",
      "INFO:ragpipeline:Connecting to Qdrant at http://localhost:6333, collection: sagemaker_docs_v1.1\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1.1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1.1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1.1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/sagemaker_docs_v1.1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1.1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1.1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1.1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://localhost:6333/collections/sagemaker_docs_v1.1/points/search \"HTTP/1.1 200 OK\"\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 1 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 1 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 2 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 2 of 4 examples\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 3 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 3 of 4 examples\n",
      "INFO:botocore.tokens:Found token in environment variables.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluated 4 of 4 examples\n",
      "INFO:weave.flow.eval:Evaluated 4 of 4 examples\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Evaluation summary {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"correctness_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"correctness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 3.25\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"relevancy_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"relevancy\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 0.4\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"faithfullness_evaluator\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"faithfullness\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:       \"mean\": 1.0\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   },\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   \"model_latency\": {\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:     \"mean\": 11.866437435150146\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:   }\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: }\n",
      "INFO:weave.flow.eval:Evaluation summary {\n",
      "  \"correctness_evaluator\": {\n",
      "    \"correctness\": {\n",
      "      \"mean\": 3.25\n",
      "    }\n",
      "  },\n",
      "  \"relevancy_evaluator\": {\n",
      "    \"relevancy\": {\n",
      "      \"mean\": 0.4\n",
      "    }\n",
      "  },\n",
      "  \"faithfullness_evaluator\": {\n",
      "    \"faithfullness\": {\n",
      "      \"mean\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"model_latency\": {\n",
      "    \"mean\": 11.866437435150146\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%pybash\n",
    "uv run python aws_doc_ragqa/run_eval.py --gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-doc-ragqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
